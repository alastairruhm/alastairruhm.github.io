<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on My New Hugo Site</title>
    <link>https://alastairruhm.github.io/post/index.xml</link>
    <description>Recent content in Posts on My New Hugo Site</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Wed, 12 Jul 2017 00:00:00 +0000</lastBuildDate>
    <atom:link href="https://alastairruhm.github.io/post/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>kubernetes 集群监控方案研究</title>
      <link>https://alastairruhm.github.io/post/kubernetes-monitoring/</link>
      <pubDate>Wed, 12 Jul 2017 00:00:00 +0000</pubDate>
      
      <guid>https://alastairruhm.github.io/post/kubernetes-monitoring/</guid>
      <description>

&lt;h1 id=&#34;kubernetes-集群监控方案研究&#34;&gt;kubernetes 集群监控方案研究&lt;/h1&gt;

&lt;h2 id=&#34;kubernetes-时代的监控新的特点&#34;&gt;kubernetes 时代的监控新的特点&lt;/h2&gt;

&lt;p&gt;监控 kubernetes 和传统监控上的一些差异&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Tags 和 labels 变得非常重要；在 kubernetes 系统中，labels 是识别 pods 和 containers 的唯一方式&lt;/li&gt;
&lt;li&gt;与传统VM监控相比，有更多的组件需要监控: 宿主机器, 容器, 容器化的应用和 kubernetes 本身&lt;/li&gt;
&lt;li&gt;容器在 kubernetes 中可能发生移动;因此需要监控系统提供服务发现的功能，检测任何来自 pod 和 容器配置的变化，自动适配监控指标的收集，以便持续的监控容器化的应用&lt;/li&gt;
&lt;li&gt;适应分布式集群监控的特点&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;kubernetes-系统中有哪些指标需要监控&#34;&gt;kubernetes 系统中有哪些指标需要监控&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;通常的资源指标，如CPU，内存使用量和磁盘IO&lt;/li&gt;
&lt;li&gt;kubernetes 各逻辑对象的状态，比如 pod 状态，deployment 更新的次数等&lt;/li&gt;
&lt;li&gt;容器的原生监控指标&lt;/li&gt;
&lt;li&gt;应用程序监控指标&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;既有方案比较&#34;&gt;既有方案比较&lt;/h2&gt;

&lt;h3 id=&#34;方案一-heapster-influxdb-grafana&#34;&gt;方案一：Heapster + influxDB + Grafana&lt;/h3&gt;

&lt;p&gt;首先这里的 Heapster 是什么？&lt;/p&gt;

&lt;p&gt;Kubernetes有个出名的监控agent&amp;mdash;cAdvisor。在每个kubernetes Node上都会运行cAdvisor，它会收集本机以及容器的监控数据(cpu,memory,filesystem,network,uptime)。在较新的版本中，K8S已经将cAdvisor功能集成到kubelet组件中。每个Node节点可以直接进行web访问。&lt;/p&gt;

&lt;p&gt;Heapster是一个收集者，将每个Node上的cAdvisor的数据进行汇总，然后导到第三方工具(如InfluxDB)。&lt;/p&gt;

&lt;p&gt;该方案的优点是 &lt;code&gt;heapster&lt;/code&gt; 是 K8s 体系原生的，不需要太多复杂配置就可以完成监控；但是反面来说，&lt;code&gt;heapster&lt;/code&gt; 局限于 kubernetes 的监控，而不是出于通用监控的目的，另外，heapster 缺少 alert 组件。&lt;/p&gt;

&lt;h3 id=&#34;方案二-prometheus-exporter-grafana&#34;&gt;方案二：prometheus + (*)-exporter + Grafana&lt;/h3&gt;

&lt;p&gt;之前我分享过 prometheus 是基于 pull 模型的监控系统，那为什么在 Kubernetes 系统的监控中是一个合理的选择，这里有几点&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;kubernetes 原生支持 prometheus：apiserver 服务的 &lt;code&gt;http://master_ip:8080/metrics&lt;/code&gt; endpoint 将集群中的监控数据暴露出来，prometheus 可以通过 pull 获取&lt;/li&gt;
&lt;li&gt;cAdvisor 原生支持 prometheus：cAdvisor 已经集成在 kubelet 服务中，prometheus 可以从 &lt;code&gt;http://node_ip:4194/metrics&lt;/code&gt; 获取监控数据&lt;/li&gt;
&lt;li&gt;prometheus 通过配置 &lt;kubernetes_sd_configs&gt;，支持将 kubernetes 作为一种服务发现机制&lt;/li&gt;
&lt;li&gt;kubernetes 可以通过 daemonset 这种资源类型来部署 &lt;code&gt;node-exporter&lt;/code&gt;，收集每个 node 通用的资源指标，如CPU，内存使用量和磁盘IO&lt;/li&gt;
&lt;li&gt;kube-state-metrics: kubernetes 各逻辑对象的状态，比如 pod 状态，deployment 更新的次数等5、&lt;/li&gt;
&lt;li&gt;应用的监控则可以通过在 Pod 部署时加入相应类型的 exporter 容器来向容器外暴露监控指标，比如在一个运行 mongodb 的 pod 中，加入一个 &lt;code&gt;mongo_exporter&lt;/code&gt;暴露mongodb的监控指标给 prometheus&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;这套方案的优点则是 Prometheus 是一个通用的监控系统，可以自由扩展，并且拥有 alertmanager 这样的功能完整的告警组件；而在 pod 中加入一个新的容器来向外暴露监控指标的部署方式又和 k8s 结合的很好。&lt;/p&gt;

&lt;h2 id=&#34;参考资料&#34;&gt;参考资料&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://blog.kubernetes.io/2017/05/kubernetes-monitoring-guide.html&#34;&gt;Kubernetes: Kubernetes: a monitoring guide&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;datadog 系列文章

&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;https://www.datadoghq.com/blog/monitoring-kubernetes-era/&#34;&gt;Monitoring in the Kubernetes era&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.datadoghq.com/blog/monitoring-kubernetes-performance-metrics/#correlate-with-events&#34;&gt;Monitoring Kubernetes performance metrics&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.datadoghq.com/blog/how-to-collect-and-graph-kubernetes-metrics/#adding-kube-state-metrics&#34;&gt;How to collect and graph Kubernetes metrics&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://dockone.io/article/1881&#34;&gt;Kubernetes监控之Heapster介绍 - DockOne.io&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.slideshare.net/brianbrazil/monitoring-kubernetes-with-prometheus-kubernetes-ireland-2016&#34;&gt;Monitoring Kubernetes with Prometheus (Kubernetes Ireland, 2016)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>监控系统 push 和 pull 模型</title>
      <link>https://alastairruhm.github.io/post/push-vs-pull-monitoring/</link>
      <pubDate>Mon, 12 Jun 2017 00:00:00 +0000</pubDate>
      
      <guid>https://alastairruhm.github.io/post/push-vs-pull-monitoring/</guid>
      <description>

&lt;h1 id=&#34;监控系统-push-和-pull-模型&#34;&gt;监控系统 push 和 pull 模型&lt;/h1&gt;

&lt;h2 id=&#34;push-模型&#34;&gt;Push 模型&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;var fooCount = 0

func foo() {
    // ... do stuff ...

    fooCount += 1
    metricsChan &amp;lt;- Metrics{&amp;quot;foo.count&amp;quot;, fooCount, CounterType}
}

var metricsChan = make(chan Metrics, 1000)

func metricsPusher() { // run as a goroutine
    for m := range metricsChan {
        // send m to the monitoring system
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;push 模型需要处理的一些问题&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Service Discovery: How does the application know where the monitoring system is located? For example, if you are reporting metrics to a StatsD server, all your app instances should know the StatsD server hostname/IP.&lt;/li&gt;
&lt;li&gt;Retry Policy: The sender should have some logic to handle intermittent network disruptions and delays.&lt;/li&gt;
&lt;li&gt;Backlog Management: In the pseudo-code above, the buffered channel had a size of 1000. When dealing with high metric volume, the sender should actively manage this backlog. Cases like production rate higher than dispatch rate, backlog filling up and memory consumption of backlog should be handled.&lt;/li&gt;
&lt;li&gt;Batching: For most systems, it is efficient to batch multiple requests into one, thereby avoiding multiple round trips. The sender should make use of batching if possible.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;pull-模型&#34;&gt;Pull 模型&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;import _ &amp;quot;expvar&amp;quot;

var fooCount = expvar.NewInt(&amp;quot;foo.count&amp;quot;)

func foo() {
    // ... do stuff ...

    fooCount.Add(1)
}

func main() {
    http.ListenAndServe(&amp;quot;:8080&amp;quot;, nil)
    // http://localhost:8080/debug/vars has the metrics
}

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;pull 模型的一些不同于 push 的特点：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Lower Application Cost: The cost of memory and CPU at application side is proportional to the number of metrics, not the rate of production of metrics.&lt;/li&gt;
&lt;li&gt;No Application-side Service Discovery: The task of discovering the HTTP endpoints to be monitored is shifted to the monitoring system side.&lt;/li&gt;
&lt;li&gt;Risk of Lost Outliers: If an outlier occurs within two pulls, it will be missed by the monitoring system.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;No Events&lt;/strong&gt;: Typically, it is not possible to report one-shot events (like a “reload” or “deploy”) using the pull mechanism.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;两类模型的代表&#34;&gt;两类模型的代表&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;prometheus 是典型的基于 pull 模型的监控系统，但是它也可以通过 pushgateway 组件支持 push 模型&lt;/li&gt;
&lt;li&gt;StatsD has mostly become the defacto standard for the push method of reporting metrics.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Golang 标准库中的 expvar 包常用来暴露 app 中的 metrics&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;furthur-reading&#34;&gt;furthur reading&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.opsdash.com/blog/golang-app-monitoring-statsd-expvar-prometheus.html&#34;&gt;Go App Monitoring: expvar, Prometheus and StatsD - OpsDash&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.influxdata.com/monitoring-with-push-vs-pull-influxdb-adds-pull-support-with-kapacitor/&#34;&gt;InfluxData Kapacitor 1.3 | Monitoring Push vs Pull and The Support of Both&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://gocn.io/question/373&#34;&gt;监控系统中到底是pull还是push方案好？ - Go 技术社区 - golang&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>git 通过代理加速</title>
      <link>https://alastairruhm.github.io/post/git-acceleration/</link>
      <pubDate>Mon, 07 Nov 2016 00:00:00 +0000</pubDate>
      
      <guid>https://alastairruhm.github.io/post/git-acceleration/</guid>
      <description>

&lt;h1 id=&#34;git-通过代理加速&#34;&gt;git 通过代理加速&lt;/h1&gt;

&lt;p&gt;github 访问受限，导致 clone / push 等操作速度很慢。以下是两个加速的方法以及缺陷和对比&lt;/p&gt;

&lt;h2 id=&#34;https&#34;&gt;https&lt;/h2&gt;

&lt;p&gt;github 允许用户通过 &lt;code&gt;https&lt;/code&gt; 端口使用 &lt;code&gt;ssh&lt;/code&gt;，可以通过下面的指令测试&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ssh -T -p 443 git@ssh.github.com
Hi username! You&#39;ve successfully authenticated, but GitHub does not
provide shell access.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;如果测试不通过，就需要修改配置文件 &lt;code&gt;~/.ssh/config&lt;/code&gt;，增加&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Host github.com
  Hostname ssh.github.com
  Port 443
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;优点&#34;&gt;优点&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;http&lt;/code&gt; 和 &lt;code&gt;https&lt;/code&gt; 代理是非常常见的，比如我一般都是对系统全局代理&lt;/li&gt;
&lt;li&gt;配置比较简单&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;缺点&#34;&gt;缺点&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;因为走的是 &lt;code&gt;https&lt;/code&gt; 协议，那么在github认证时只能使用提供 username/password 的方式认证，如果要避免每次push时都输入密码，需要一些额外的步骤。&lt;a href=&#34;https://help.github.com/articles/caching-your-github-password-in-git/&#34;&gt;Caching your GitHub password in Git - User Documentation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;另外如果开启了 &lt;code&gt;two-factor authentication&lt;/code&gt;，还需要提供 &lt;code&gt;personal access token&lt;/code&gt;。&lt;a href=&#34;https://help.github.com/articles/https-cloning-errors/#provide-access-token-if-2fa-enabled&#34;&gt;Provide access token if 2FA enabled - User Documentation&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;更多详情参考 &lt;a href=&#34;https://help.github.com/articles/using-ssh-over-the-https-port/&#34;&gt;Using SSH over the HTTPS port - User Documentation&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;ssh&#34;&gt;ssh&lt;/h2&gt;

&lt;p&gt;前提：需要一个 socks5 代理&lt;/p&gt;

&lt;p&gt;如果是直接通过 &lt;code&gt;ssh&lt;/code&gt; 协议访问，则需要按照以下步骤配置&lt;/p&gt;

&lt;p&gt;在 &lt;code&gt;/usr/local/bin&lt;/code&gt; 增加一个文件，名为 &lt;code&gt;git-proxy-wrapper&lt;/code&gt;，增加 +x 权限&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ruhm@mac:~$ cat /usr/local/bin/git-proxy-wrapper
#! /bin/bash
nc -xlocalhost:1080 -X5 $*
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;注意，上面需要本机在 &lt;code&gt;1080&lt;/code&gt; 端口打开 &lt;code&gt;socks5&lt;/code&gt; 代理，端口可以自定义&lt;/p&gt;

&lt;p&gt;&lt;code&gt;~/.ssh/config&lt;/code&gt; 配置文件中内容如下&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Host github.com
    hostname github.com
    User xxxxx
    IdentityFile ~/.ssh/xxxxxx
    ProxyCommand /usr/local/sbin/git-proxy-wrapper &#39;%h %p&#39;
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;优点-1&#34;&gt;优点&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;ssh 认证不需要提供用户名和密码&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;缺点-1&#34;&gt;缺点&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;需要一个 &lt;code&gt;socks5&lt;/code&gt; 代理&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;参考&#34;&gt;参考&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.zhihu.com/question/23315073&#34;&gt;有何方法可以给github远程仓库的push提速？ - 知乎&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://gist.github.com/goncha/4591538&#34;&gt;Git and socks5 proxy&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
  </channel>
</rss>